{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nHello this is the code for decision tree implemenntaion. What I have done is I have created the tree and then also predicted\\nthe values. I have taken the iris dataset as instructed and then split it into train and test data. Now using the train data,\\nI have developed a tree. For the tree I have created a class Node which contains all the feature of that node like x_data, \\ny_data, entropy, probabilities, count of class 0, 1, 2 in Y. For splitting the data I used the median values of the data \\nfeaturewise and split it into two halves. I have created this tree for 2 splits only, 1 which contains values greater than \\nor equal to the mean and the other which contains values smaller. Then I returned the head node from the run function and \\nused the predict function to predict values(y_pred) and compared to y_test. For the 38 test values the answer varies from \\n34/38 correct to 36/38 correct depending on the random states used while splitting the iris data.\\nA picture of the tree created can be found here:\\nhttps://app.creately.com/diagram/Gdrtjxiy3BY\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hello this is the code for decision tree implemenntaion. What I have done is I have created the tree and then also predicted\n",
    "the values. I have taken the iris dataset and then split it into train and test data. Now using the train data,\n",
    "I have developed a tree. For the tree I have created a class Node which contains all the feature of that node like x_data, \n",
    "y_data, entropy, probabilities, count of class 0, 1, 2 in Y. For splitting the data I used the median values of the data \n",
    "featurewise and split it into two halves. I have created this tree for 2 splits only, 1 which contains values greater than \n",
    "or equal to the mean and the other which contains values smaller. Then I returned the head node from the run function and \n",
    "used the predict function to predict values(y_pred) and compared to y_test. For the 38 test values the answer varies from \n",
    "34/38 correct to 36/38 correct depending on the random states used while splitting the iris data.\n",
    "A picture of the tree created can be found here:\n",
    "https://app.creately.com/diagram/Gdrtjxiy3BY\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "112 1.5848478277058315 [37 37 38] [0.33035714 0.33035714 0.33928571] 4 [1 2 3 4] 1.3\n66 0.9833761901392237 [ 0 28 38] [0.         0.42424242 0.57575758] 3 [1 2 3] 5.0\n34 0.19143325481419343 [ 0  1 33] [0.         0.02941176 0.97058824] 1 [1 2] 6.5\n19 0.2974722489192896 [ 0  1 18] [0.         0.05263158 0.94736842] 1 [2] 6.8\n10 0.0 [ 0  0 10] [0. 0. 1.] 0 [2] 0\n9 0.5032583347756457 [0 1 8] [0.         0.11111111 0.88888889] 1 [2] 6.7\n5 0.7219280948873623 [0 1 4] [0.  0.2 0.8] 1 [2] 6.7\n4 0.0 [0 0 4] [0. 0. 1.] 0 [2] 0\n15 0.0 [ 0  0 15] [0. 0. 1.] 0 [2] 0\n32 0.6252624052234231 [ 0 27  5] [0.      0.84375 0.15625] 2 [1 2] 2.9\n19 0.4854607607459134 [ 0 17  2] [0.         0.89473684 0.10526316] 1 [1] 6.1\n11 0.4394969869215134 [ 0 10  1] [0.         0.90909091 0.09090909] 0 [] 0\n8 0.5435644431995964 [0 7 1] [0.    0.875 0.125] 0 [] 0\n13 0.7793498372920852 [ 0 10  3] [0.         0.76923077 0.23076923] 1 [1] 5.7\n8 0.8112781244591328 [0 6 2] [0.   0.75 0.25] 0 [] 0\n5 0.7219280948873623 [0 4 1] [0.  0.8 0.2] 0 [] 0\n46 0.7131467486384921 [37  9  0] [0.80434783 0.19565217 0.        ] 2 [1 2 3] 3.2\n26 0.0 [26  0  0] [1. 0. 0.] 0 [1 3] 0\n20 0.9927744539878084 [11  9  0] [0.55 0.45 0.  ] 1 [1 3] 4.95\n10 0.4689955935892812 [1 9 0] [0.1 0.9 0. ] 1 [3] 5.6\n5 0.0 [0 5 0] [0. 1. 0.] 0 [3] 0\n5 0.7219280948873623 [1 4 0] [0.2 0.8 0. ] 1 [3] 5.5\n3 0.0 [0 3 0] [0. 1. 0.] 0 [3] 0\n2 1.0 [1 1 0] [0.5 0.5 0. ] 1 [3] 5.0\n10 0.0 [10  0  0] [1. 0. 0.] 0 [3] 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.,  0., -1.,  0.,  0.,  0.,  1.,  0.,  0., -1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
       "        0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def print_tree(dtreehead):\n",
    "    dtreehead.printselfdata()\n",
    "    if(type(dtreehead.rightsplit)==int):\n",
    "        return\n",
    "    if(type(dtreehead.leftsplit)==int):\n",
    "        return\n",
    "    if(dtreehead.is_leaf==1):\n",
    "        return\n",
    "    print_tree(dtreehead.leftsplit)\n",
    "    print_tree(dtreehead.rightsplit)\n",
    "def entropy_calc(dtreehead):\n",
    "    dtreehead.entropycalc()\n",
    "    if(type(dtreehead.rightsplit)==int):\n",
    "        return\n",
    "    if(type(dtreehead.leftsplit)==int):\n",
    "        return\n",
    "    if(dtreehead.is_leaf==1):\n",
    "        return\n",
    "    entropy_calc(dtreehead.leftsplit)\n",
    "    entropy_calc(dtreehead.rightsplit)\n",
    "def splits(x_train, y_train, feature):\n",
    "    split1x=[]\n",
    "    split1y=[]\n",
    "    split2x=[]\n",
    "    split2y=[]\n",
    "    for i in range(x_train.shape[0]):\n",
    "        if x_train[i][feature]>=np.median(x_train[:,feature]):\n",
    "            split1x.append(x_train[i])\n",
    "            split1y.append(y_train[i])\n",
    "        else:\n",
    "            split2x.append(x_train[i])\n",
    "            split2y.append(y_train[i])\n",
    "    split1x=np.array(split1x)\n",
    "    split1y=np.array(split1y)\n",
    "    split2x=np.array(split2x)\n",
    "    split2y=np.array(split2y)\n",
    "    return split1x, split1y, split2x, split2y\n",
    "def createtree(dtreehead):\n",
    "    if(dtreehead.is_leaf==1):\n",
    "        return\n",
    "    info_gain=np.zeros(dtreehead.features.shape[0])\n",
    "    curr=0\n",
    "    for i in dtreehead.features:\n",
    "        temp=np.delete(dtreehead.features,np.where(dtreehead.features==i))\n",
    "        split1x,split1y,split2x,split2y=splits(dtreehead.x_data, dtreehead.y_data, i-1)\n",
    "        left_split=node(split1x,split1y, temp)\n",
    "        right_split=node(split2x, split2y, temp)\n",
    "        left_split.entropycalc()\n",
    "        right_split.entropycalc()\n",
    "        leftratio=left_split.no_of_elements/dtreehead.no_of_elements\n",
    "        rightratio=right_split.no_of_elements/dtreehead.no_of_elements\n",
    "        num=(dtreehead.entropy-(leftratio*left_split.entropy)-(rightratio*right_split.entropy))\n",
    "        den=((-1)*((leftratio*math.log2(leftratio))+(rightratio*math.log2(rightratio))))\n",
    "        info_gain[np.where(dtreehead.features==i)]=num/den\n",
    "    curr=np.argmax(info_gain)+1\n",
    "    temp=np.delete(dtreehead.features, np.where(dtreehead.features==curr))\n",
    "    dtreehead.feature=curr\n",
    "    dtreehead.median=(np.median(dtreehead.x_data[:,curr-1]))\n",
    "    split1x,split1y,split2x,split2y=splits(dtreehead.x_data, dtreehead.y_data, curr-1)\n",
    "    if(split1x.shape[0]*split2x.shape[0]!=0):\n",
    "        dtreehead.leftsplit=node(split1x,split1y, temp)\n",
    "    if(split1x.shape[0]*split2x.shape[0]!=0):\n",
    "        dtreehead.rightsplit=node(split2x, split2y, temp)\n",
    "    if(dtreehead.is_leaf==1):\n",
    "        return\n",
    "    if(split1x.shape[0]*split2x.shape[0]!=0):\n",
    "        createtree(dtreehead.leftsplit)\n",
    "    if(split1x.shape[0]*split2x.shape[0]!=0):\n",
    "        createtree(dtreehead.rightsplit)\n",
    "class node:\n",
    "    def __init__(self, x_train, y_train, features):\n",
    "        self.x_data=x_train\n",
    "        self.y_data=y_train\n",
    "        self.cz=0\n",
    "        self.co=0\n",
    "        self.ct=0\n",
    "        self.features=features\n",
    "        self.no_of_elements=x_train.shape[0]\n",
    "        self.leftsplit=0\n",
    "        self.rightsplit=0\n",
    "        self.feature=0\n",
    "        self.median=0\n",
    "        self.prob=[]\n",
    "        if(len(features)==0 or len(set(y_train))==1):\n",
    "            self.is_leaf=1\n",
    "        else:\n",
    "            self.is_leaf=0\n",
    "        self.entropy=0\n",
    "#         print(self.x_data, self.y_data)\n",
    "    def entropycalc(self):\n",
    "        for i in range(len(self.y_data)):\n",
    "            if self.y_data[i]==1:\n",
    "                self.co+=1;\n",
    "            elif self.y_data[i]==2:\n",
    "                self.ct+=1;\n",
    "            else:\n",
    "                self.cz+=1;\n",
    "        self.counts=np.array([self.cz,self.co,self.ct])\n",
    "        self.pz=self.cz/self.no_of_elements\n",
    "        self.po=self.co/self.no_of_elements\n",
    "        self.pt=self.ct/self.no_of_elements\n",
    "        self.prob=np.array([self.pz,self.po,self.pt])\n",
    "#         print(prob)\n",
    "        for i in range(3):\n",
    "            if(self.prob[i]!=0):\n",
    "                self.entropy+=(self.prob[i]*math.log2(self.prob[i]))\n",
    "        if(self.entropy!=0):\n",
    "            self.entropy=self.entropy*(-1)\n",
    "    def printselfdata(self):\n",
    "        print(self.x_data.shape[0], self.entropy, self.counts, self.prob,self.feature, self. features,self.median)\n",
    "def run(x_train, y_train):\n",
    "    dtreehead=node(x_train, y_train, np.arange(1,x_train.shape[1]+1))\n",
    "    createtree(dtreehead)\n",
    "    return dtreehead\n",
    "#     y_pred=predict(x_test,y_test)\n",
    "#     print(confusion_matrix(y_test, y_test_pred))\n",
    "def helper(x_testa, dtreehead):\n",
    "    if(type(dtreehead.rightsplit)==int):\n",
    "        return np.argmax(dtreehead.counts)\n",
    "    if(type(dtreehead.leftsplit)==int):\n",
    "        return np.argmax(dtreehead.counts)\n",
    "    if(dtreehead.is_leaf==1):\n",
    "        return np.argmax(dtreehead.counts)\n",
    "    if(x_testa[dtreehead.feature-1]>=dtreehead.median):\n",
    "        return helper(x_testa, dtreehead. leftsplit)\n",
    "    return helper(x_testa, dtreehead.rightsplit)\n",
    "def predict(x_test, dtreehead):\n",
    "    y_pred=np.zeros(x_test.shape[0])\n",
    "    for i in range(x_test.shape[0]):\n",
    "        curr=helper(x_test[i],dtreehead)\n",
    "        y_pred[i]=curr\n",
    "    return y_pred\n",
    "iris=datasets.load_iris()\n",
    "x_train, x_test, y_train, y_test=train_test_split(iris.data, iris.target)\n",
    "head=run(x_train, y_train)\n",
    "entropy_calc(head)\n",
    "print_tree(head)\n",
    "y_pred=predict(x_test, head)\n",
    "y_pred-y_test"
   ]
  }
 ]
}